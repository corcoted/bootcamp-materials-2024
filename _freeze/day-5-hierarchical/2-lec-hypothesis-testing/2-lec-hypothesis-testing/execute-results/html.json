{
  "hash": "72be5e4ca26e7e4bfaf87f04b30c75fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Inference: frequentist vs. Bayesian\"\nsubtitle: \"Day 5\"\nformat: \n  revealjs:\n    slide-number: true\n    incremental: true\n    theme: [\"../../templates/slides-style.scss\"]\n    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png\n    title-slide-attributes: \n      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png\n      data-background-size: 12%\n      data-background-position: 50% 95%\n    include-after-body: ../../templates/clean_title_page.html\n---\n\n\n\n\n\n## \"Statistical inference\"\n\n- Hypothesis Testing a la frequentist\n\n- Confidence Intervals\n\n- Hypothesis Testing a la Bayes\n\n- Credible Intervals\n\n\n## Resources\n\n- Frequentist examples are from [stats4cs.com](https://www.stats4cs.com/)\n\n- Bayesian examples are from [Bayes Rules! Section 8.2](https://www.bayesrulesbook.com/chapter-8.html#posterior-hypothesis-testing)\n\n# Frequentist Inference\n\n## Research Question\n\nAre there any pink cows in the world?\n\n\n## Hypotheses\n\n**Null** hypothesis: There are __no__ pink cows in the world.  \n\n**Alternative** hypothesis: There is a pink cow in the world.\n\n\n## Hypothesis Testing Procedure\n\nWe go looking for evidence against the null. \n\n- If we find any evidence against the null (a single pink cow) then we say we **reject the null** hypothesis.\n\n- If we do not find any evidence against the null (a single pink cow) then **we fail to reject the null**. We can keep searching for more evidence against the null (i.e. continue looking for a pink cow). We will never be able to say the null is true so we never accept the null. All we can do is keep looking for a pink cow.\n\n\n\n## Research Question\n\nIs there a foreign object in the cat's body?\n\n\n\n\n## Hypothesis Testing\n\n**Null** hypothesis: There is __no__ foreign object in the cat's body.\n\n**Alternative** hypothesis: There  is a foreign object in the cat's body.\n\n\n\n## Collect Evidence\n\nX-ray\n\n\n\n## Conclusion and Decision\n\n\nX-ray does not show any foreign object. \n\n\n- Fail to reject the null hypothesis.\n- We __cannot__ conclude the null hypothesis is true. We __cannot__ accept the null hypothesis.\n\n\n\n## Review of Hypothesis Testing\n\n- We assume that the null hypothesis is true.\n\n- We look for evidence against the null.\n\n- If we find any evidence against the null (e.g. a single pink cow) then we can say we **reject the null hypothesis**.\n\n- If we do not find any evidence against the null (a single pink cow) then we fail to reject the null. We can keep searching for more evidence against the null (i.e. continue looking for a pink cow). We will never be able to say the null is true so we never accept the null. We **fail to reject the null**. All we can do is keep looking for a pink cow.\n\n##\n\nWe are searching for evidence against the null. \nWe are searching for samples that are _significantly_ different than the null.\n\n## Research Question\n\nDo the majority of Americans approve allowing DACA immigrants to become citizens? Survey about this topic can be found [here](https://news.gallup.com/poll/235775/americans-oppose-border-walls-favor-dealing-daca.aspx)\n\n\n## Hypotheses \n\n$H_0: \\pi = 0.5$  \n$H_A: \\pi \\neq 0.5$\n\n\n## Assuming Null is True\n\nRecall that according to CLT $p \\sim \\text{approximately }N(\\pi, \\frac{\\pi(1-\\pi)}{n})$\n\n. . .\n\nIf $H_0: \\pi = 0.5$ then the null sampling distribution would be $N(0.5, \\frac{0.5(1-0.5)}{n})$\n\n\n\n\n## Looking for Evidence \n\n\nAccording to a Gallup survey of 1520 US adults , 83% approve of allowing DACA immigrants to become citizens.\n\n\n. . .\n\n$p = 0.83$  \n$n = 1520$\n\n. . .\n\nWe said that the null sampling distribution would be $N(0.5, \\frac{0.5(1-0.5)}{n})$ which is\n\n$N(0.5, \\frac{0.5(1-0.5)}{1520})$\n\n$N(0.5, 0.0001644737)$\n\n\n## The $H_0$ Sampling Distribution \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## What counts as evidence against the null?\n\nAny sample proportion that falls of really far away from the center of the distribution would count as an evidence against the null.\n\nIf the null is true, then it would be unlikely to observe extremely high or low sample proportions. \n\n\n\n## Sampling Distribution\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n##\n\nWe want to know the probability of observing an extreme sample proportion like ours (p = 0.83) if the $H_0$ were true.\n\n. . .\n\nIf our sample proportion is \"extreme\" then so is 0.90, 0.91, 0.917, 0.9273423 etc. \n\n. . .\n\nOur sample proportion is 0.83 - 0.5 = 0.33 units away from the null.\n\n. . .\n\nSo we will consider 0.5 - 0.33 = 0.17 also an \"extreme\" sample proportion. \n\n. . .\n\nThis makes 0.16, 0.1512, 0.11... also \"extreme\" \n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\nIf the $H_0$ is true what is the probability that we will observe an extremely high or an extremely low sample proportion?\n\nProbability of observing sample proportion of 0.17 and lower \n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(0.17, mean = 0.5, sd = 0.01282473)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.594241e-146\n```\n\n\n:::\n:::\n\n\n\n##\n\nProbability of observing sample proportion of 0.83 and higher \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(0.83, mean = 0.5, sd = 0.01282473, \n      lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.594241e-146\n```\n\n\n:::\n:::\n\n\n\n\n\n## p-value\n\n\nAdding those up (or you can multiply one of them with 2) we have\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(0.17, mean = 0.5, sd = 0.01282473) + \n  pnorm(0.83, mean = 0.5, sd = 0.01282473, \n        lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.188482e-146\n```\n\n\n:::\n:::\n\n\n\n. . .\n\np-value  = $5.188482 \\times 10^{-146}$\n\n. . .\n\nP-value is the probability of observing a sample statistic at least as extreme as the one that has been observed if the null hypothesis were true.\n\n\n\n\n## Remembering CLT\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\nLet $\\pi$ represent the proportion of bike owners on campus then $\\pi =$ 0.15. \n\n\n\n## Getting to sampling distribution of single proportion\n\n$p_1$ - Proportion of first sample (n = 100)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.17\n```\n\n\n:::\n:::\n\n\n\n$p_2$ -Proportion of second sample (n = 100)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.12\n```\n\n\n:::\n:::\n\n\n\n$p_3$ -Proportion of third sample (n = 100)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.14\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n### Sampling Distribution of Single Proportion\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\nIf certain conditions are met then\n\n$$p \\sim \\text{approximately } N(\\pi, \\frac{\\pi(1-\\pi)}{n})$$\n\n\n## In Reality\n\n- We only have one sample and thus one point estimate of the population parameter. How can make use of it? \n\n. . .\n\n- First we will assume the sample proportion is the best thing we have at hand and use it as a point estimate of the population proportion. \n\n. . .\n\n- Second, even though we embrace the sample proportion as a point estimate of the population proportion, we will need to acknowledge that it has some error. \n\n\n## Standard Error\n\n\n$p \\sim  \\text{approximately } N(\\text{mean} = \\pi, \\text{sd} = \\sqrt{\\frac{\\pi(1-\\pi)}{n}})$\n\n. . .\n\nWe call the standard deviation of the sampling distribution __standard error__ of the estimate. \n\nStandard error of single proportion is $\\sqrt{\\frac{p(1-p)}{n}}$.\n\n\n\n## Confidence Interval \n\nCI = $\\text{point estimate} \\pm \\text { margin of error}$\n\n. . .\n\nCI = $\\text{point estimate} \\pm \\text { critical value} \\times \\text{standard error}$\n\n. . .\n\nCI for single proportion = $p \\pm \\text {critical value} \\times \\text{standard error}$\n\n. . .\n\nCI for single proportion = $p \\pm \\text {critical value} \\times \\sqrt{\\frac{p(1-p)}{n}}$\n\n. . .\n\n95% CI for single proportion = $p \\pm 1.96 \\times \\sqrt{\\frac{p(1-p)}{n}}$ because ...\n\n##\n\n95% of the data falls within 1.96 standard deviations in the normal distribution.\n      \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n      \n\n## How do we know that?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqnorm(0.025, mean = 0 , sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.959964\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqnorm(0.975, mean = 0 , sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n:::\n\n\n\n\n## 95% CI for the first sample\n\nRecall $p = 0.17$ and $n = 100$\n\n. . .\n\n95% CI for single proportion = $p \\pm 1.96 \\times \\sqrt{\\frac{p(1-p)}{n}}$ \n\n. . .\n\n95% CI = $0.17 \\pm 1.96 \\times \\sqrt{\\frac{0.17(1-0.17)}{100}}$ \n\n. . .\n\n95% CI = $0.17 \\pm 1.96 \\times 0.03756328$ \n\n. . .\n\n95%CI = $0.17 \\pm 0.07362403$\n\n. . .\n\n95%CI = (0.09637597, 0.243624)\n\n\n\n## 95% CI for the first sample\n\n95%CI = (0.09637597, 0.243624)\n\nWe are 95% confident that the true population proportion of bike owners is in this confidence interval.\n\nclass: middle center\n\n95%CI = (0.09637597, 0.243624)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Understanding Confidence Intervals\n\n\nI have taken 100 samples with $n = 100$, calculated the sample proportion, standard error, and 95% CI interval for each sample\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 4\n       p     SE lower_bound upper_bound\n   <dbl>  <dbl>       <dbl>       <dbl>\n 1  0.19 0.0392      0.113        0.267\n 2  0.21 0.0407      0.130        0.290\n 3  0.15 0.0357      0.0800       0.220\n 4  0.15 0.0357      0.0800       0.220\n 5  0.13 0.0336      0.0641       0.196\n 6  0.11 0.0313      0.0487       0.171\n 7  0.16 0.0367      0.0881       0.232\n 8  0.11 0.0313      0.0487       0.171\n 9  0.19 0.0392      0.113        0.267\n10  0.16 0.0367      0.0881       0.232\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Understanding Confidence Intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Understanding Confidence Intervals\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n##\n\n- The confidence interval can be expressed in terms of a long-run frequency in repeated samples (or in resampling): \"Were this procedure to be repeated on numerous samples, the proportion of calculated 95% confidence intervals that encompassed the true value of the population parameter would tend toward 95%.\"\n\n- The confidence interval can be expressed in terms of probability with respect to a single theoretical (yet to be realized) sample: \"There is a 95% probability that the 95% confidence interval calculated from a given future sample will cover the true value of the population parameter.\"\nThis essentially reframes the \"repeated samples\" interpretation as a probability rather than a frequency.\n\n- The confidence interval can be expressed in terms of statistical significance, e.g.: \"The 95% confidence interval represents values that are not statistically significantly different from the point estimate at the .05 level.\"\n\n# Bayesian Inference \n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bayesrules)\ndata(moma_sample)\nglimpse(moma_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 10\n$ artist            <fct> Ad Gerritsen, Kirstine Roepstorff, Lisa Baumgardner,…\n$ country           <fct> dutch, danish, american, american, american, canadia…\n$ birth             <fct> 1940, 1972, 1958, 1952, 1946, 1927, 1901, 1941, 1920…\n$ death             <fct> 2015, NA, 2015, NA, NA, 1966, 1971, NA, 2007, NA, NA…\n$ alive             <lgl> FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, …\n$ genx              <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ gender            <fct> male, female, female, male, male, male, male, male, …\n$ count             <int> 1, 3, 2, 1, 1, 8, 2, 1, 1, 5, 1, 2, 1, 1, 21, 16, 1,…\n$ year_acquired_min <fct> 1981, 2005, 2016, 2001, 2012, 2008, 2018, 1981, 1949…\n$ year_acquired_max <fct> 1981, 2005, 2016, 2001, 2012, 2008, 2018, 1981, 1949…\n```\n\n\n:::\n:::\n\n\n\n$$Y|\\pi  \\sim \\text{Bin}(100, \\pi)$$ \n\n$$\\pi  \\sim \\text{Beta}(4, 6)$$\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmoma_sample %>% \n  mutate(genx = (birth >= 1965)) %>% \n  janitor::tabyl(genx)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n genx   n percent valid_percent\n   NA 100       1            NA\n```\n\n\n:::\n:::\n\n\n\n##\n\n$$\\begin{split}\nY | \\pi & \\sim \\text{Bin}(100, \\pi) \\\\\n\\pi & \\sim \\text{Beta}(4, 6) \\\\\n\\end{split} \\;\\;\\;\\; \\Rightarrow \\;\\;\\;\\; \\pi | (Y = 14) \\sim \\text{Beta}(18, 92)$$\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 4, beta = 6, y = 14, n = 100)\n```\n\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummarize_beta_binomial(4, 6, y = 14, n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      model alpha beta      mean      mode         var         sd\n1     prior     4    6 0.4000000 0.3750000 0.021818182 0.14770979\n2 posterior    18   92 0.1636364 0.1574074 0.001232969 0.03511365\n```\n\n\n:::\n:::\n\n\n\n##\n\n## 95% Posterior Credible Interval (CI)\n\n0.025th & 0.975th quantiles of the Beta(18,92) posterior\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqbeta(c(0.025, 0.975), 18, 92)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1009084 0.2379286\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n$\\int_{0.1}^{0.24} f(\\pi|(y=14)) d\\pi  \\; \\approx \\; 0.95$\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/post-ci-ch8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n$$\\begin{split}\nH_0: & \\; \\; \\pi \\ge 0.20 \\\\\nH_a: & \\; \\; \\pi < 0.20 \\\\\n\\end{split}$$\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/post-prob-ch8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Posterior probability that pi < 0.20\npost_prob <- pbeta(0.20, 18, 92)\npost_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8489856\n```\n\n\n:::\n:::\n\n\n\n##\n\n$$\\text{Posterior odds } = \\frac{P(H_a \\; | \\; (Y=14))}{P(H_0 \\; | \\; (Y=14))} \\approx 5.62 $$  \n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Posterior odds\npost_odds <- post_prob / (1 - post_prob)\npost_odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.621883\n```\n\n\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/prior-post-ch8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n$$P(\\pi<0.20)$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprior_prob <- pbeta(0.20, 4, 6)\nprior_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08564173\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n$$\\text{Prior odds } = \\frac{P(H_a)}{P(H_0)} \\approx 0.093 \\; .$$ \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprior_odds <- prior_prob / (1 - prior_prob)\nprior_odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09366321\n```\n\n\n:::\n:::\n\n\n\n##\n\n\nThe __Bayes Factor (BF)__ compares the posterior odds to the prior odds, hence provides insight into just how much our understanding about Gen X representation _evolved_ upon observing our sample data:\n\n\n$$\\text{Bayes Factor} = \\frac{\\text{Posterior odds }}{\\text{Prior odds }}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Bayes Factor\n\nIn a hypothesis test of two competing hypotheses, $H_a$ vs $H_0$, the Bayes Factor is an odds ratio for $H_a$:\n\n$$\\text{Bayes Factor}\n= \\frac{\\text{Posterior odds}}{\\text{Prior odds}}\n= \\frac{P(H_a | Y) / P(H_0 | Y)}{P(H_a) / P(H_0)}\n \\; .$$\n\nAs a ratio, it's meaningful to compare the Bayes Factor (BF)\\ to 1.  To this end, consider three possible scenarios:\n\n1. BF = 1:  The plausibility of $H_a$ _didn't change_ in light of the observed data.\n2. BF > 1:  The plausibility of $H_a$ _increased_ in light of the observed data.  Thus the greater the Bayes Factor, the more convincing the evidence for $H_a$.\n3. BF < 1:  The plausibility of $H_a$ _decreased_ in light of the observed data. \n\n\n## Two-sided Tests\n\n$$\\begin{split}\nH_0: & \\; \\; \\pi = 0.3 \\\\\nH_a: & \\; \\; \\pi \\ne 0.3 \\\\\n\\end{split}$$\n\n$$\\text{Posterior odds } = \\frac{P(H_a \\; | \\; (Y=14))}{P(H_0 \\; | \\; (Y=14))} = \\frac{1}{0} = \\text{ nooooo!}$$\n\n. . .\n\nRecall 95% posterior CI\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1009084 0.2379286\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 1:                0.037 seconds (Sampling)\nChain 1:                0.07 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 2:                0.042 seconds (Sampling)\nChain 2:                0.076 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 3:                0.049 seconds (Sampling)\nChain 3:                0.084 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.072 seconds (Warm-up)\nChain 4:                0.043 seconds (Sampling)\nChain 4:                0.115 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bayesplot)\n# Parallel trace plots & density plots\nmcmc_trace(art_sim, pars = \"pi\", size = 0.5)\n```\n\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-34-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-35-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-38-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-lec-hypothesis-testing_files/figure-revealjs/unnamed-chunk-39-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n  post_mean post_mode lower_95  upper_95\n1 0.1635274 0.1532068 0.100797 0.2387512\n```\n\n\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n exceeds     n percent\n   FALSE  3011 0.15055\n    TRUE 16989 0.84945\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8489856\n```\n\n\n:::\n:::\n\n\n\n\n__a Bayesian analysis assesses the uncertainty regarding an unknown parameter $\\pi$ in light of observed data $Y$__.\n\n\n$$P((\\pi < 0.20) \\; | \\; (Y = 14)) = 0.8489856 \\;.$$\n\n. . .\n\n__a frequentist analysis assesses the uncertainty of the observed data $Y$ in light of assumed values of $\\pi$.__\n\n$$P((Y \\le 14) | (\\pi = 0.20)) = 0.08$$\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}