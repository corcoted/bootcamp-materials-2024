{
  "hash": "feefdf333e8449610105b61e50751974",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Fitting regression models\"\nsubtitle: \"Day 4\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    slide-number: true\n    incremental: true\n    theme: [\"../../templates/slides-style.scss\"]\n    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png\n    title-slide-attributes: \n      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png\n      data-background-size: 12%\n      data-background-position: 50% 85%\n---\n\n\n\nNote that examples in this lecture are a simplified version of [Chapter 9 of Bayes Rules! book](https://www.bayesrulesbook.com/chapter-9.html).\n\n\n\n## Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(broom.mixed)\ntheme_set(theme_gray(base_size = 18)) # change default font size in ggplot\n```\n:::\n\n\n\n## Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(bikes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 500\nColumns: 13\n$ date        <date> 2011-01-01, 2011-01-03, 2011-01-04, 2011-01-05, 2011-01-0…\n$ season      <fct> winter, winter, winter, winter, winter, winter, winter, wi…\n$ year        <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011…\n$ month       <fct> Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan…\n$ day_of_week <fct> Sat, Mon, Tue, Wed, Fri, Sat, Mon, Tue, Wed, Thu, Fri, Sat…\n$ weekend     <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALS…\n$ holiday     <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, yes, n…\n$ temp_actual <dbl> 57.39952, 46.49166, 46.76000, 48.74943, 46.50332, 44.17700…\n$ temp_feel   <dbl> 64.72625, 49.04645, 51.09098, 52.63430, 50.79551, 46.60286…\n$ humidity    <dbl> 80.5833, 43.7273, 59.0435, 43.6957, 49.8696, 53.5833, 48.2…\n$ windspeed   <dbl> 10.749882, 16.636703, 10.739832, 12.522300, 11.304642, 17.…\n$ weather_cat <fct> categ2, categ1, categ1, categ1, categ2, categ2, categ1, ca…\n$ rides       <int> 654, 1229, 1454, 1518, 1362, 891, 1280, 1220, 1137, 1368, …\n```\n\n\n:::\n:::\n\n\n\n\n## Rides\n\n:::: {.columns}\n\n:::{.column width=\"60%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n:::{.column width=\"40%\"}\n\n$Y_i | \\mu, \\sigma  \\stackrel{ind}{\\sim} N(\\mu, \\sigma^2)$  \n$\\mu \\sim N(\\theta, \\tau^2)$\n$\\sigma  \\sim \\text{ some prior model.}$\n\n:::\n\n::::\n\n## Regression Model\n\n$Y_i$ the number of rides  \n$X_i$ temperature (in Fahrenheit) on day $i$. \n\n. . .\n\n$\\mu_i = \\beta_0 + \\beta_1X_i$\n\n. . .\n\n$\\beta_0:$ the typical ridership on days in which the temperature was 0 degrees ( $X_i$=0). It is not interpretable in this case.\n\n. . . \n\n$\\beta_1:$ the typical change in ridership for every one unit increase in temperature.\n\n\n\n## Normal likelihood model\n\n\\begin{split}\nY_i | \\beta_0, \\beta_1, \\sigma & \\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu_i = \\beta_0 + \\beta_1X_i \\; .\\\\\n\\end{split}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\n\n##\n\nFrom previous slide\n\nBoth the model lines show cases where $\\beta_0 = -2000$ and slope $\\beta_1 = 100$.\nOn the left $\\sigma = 2000$ and on the right $\\sigma = 200$ (right). In both cases, the model line is defined by $\\beta_0 + \\beta_1 x = -2000 + 100 x$.\n\n## Model\n\n$\\text{likelihood:} \\; \\; \\; Y_i | \\beta_0, \\beta_1, \\sigma \\;\\;\\;\\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right)\\text{ with } \\mu_i = \\beta_0 + \\beta_1X_i$\n\n$\\text{prior models:}$ \n\n$\\beta_0\\sim N(m_0, s_0^2 )$  \n$\\beta_1\\sim N(m_1, s_1^2 )$  \n$\\sigma \\sim \\text{Exp}(l)$\n\n. . .\n\nNote: \n\n$\\text{Exp}(l) = \\text{Gamma}(1, l)$\n\n\n## Model building: One step at a time \n\nLet $Y$ be a response variable and $X$ be a predictor or set of predictors. Then we can build a model of $Y$ by $X$ through the following general principles:\n\n- Take note of whether $Y$ is discrete or continuous. Accordingly, identify an appropriate model structure of data $Y$ (e.g., Normal, Poisson, Binomial).\n- Rewrite the mean of $Y$ as a function of predictors $X$ (e.g., $\\mu = \\beta_0 + \\beta_1 X$).\n- Identify all unknown model parameters in your model (e.g., $\\beta_0, \\beta_1, \\sigma$).\n- Take note of the values that each of these parameters might take. Accordingly, identify appropriate prior models for these parameters.\n\n##\n\nSuppose we have the following prior understanding of this relationship:\n\n1. On an _average_ temperature day, say 65 or 70 degrees for D.C., there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.\n\n2. For every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.\n\n3. At any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_normal(mean = 5000, sd = 1000) + \n  labs(x = \"beta_0c\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_normal(mean = 100, sd = 40) + \n  labs(x = \"beta_1\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_gamma(shape = 1, rate = 0.0008) + \n  labs(x = \"sigma\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n##\n\n$$\\begin{split}\nY_i | \\beta_0, \\beta_1, \\sigma & \\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu_i = \\beta_0 + \\beta_1X_i \\\\\n\\beta_{0c}  & \\sim N\\left(5000, 1000^2 \\right)  \\\\\n\\beta_1  & \\sim N\\left(100, 40^2 \\right) \\\\\n\\sigma   & \\sim \\text{Exp}(0.0008)  .\\\\\n\\end{split}$$\n\n##\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-8-1.png){width=1440}\n:::\n:::\n\n\n\n\n## Simulation via `rstanarm`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|cache: true\nbike_model <- stan_glm(rides ~ temp_feel, data = bikes,\n                       family = gaussian,\n                       prior_intercept = normal(5000, 1000),\n                       prior = normal(100, 40), \n                       prior_aux = exponential(0.0008),\n                       chains = 4, iter = 5000*2, seed = 84735,\n                       refresh = FALSE) \n```\n:::\n\n\n\nThe `refresh = FALSE` prevents printing out your chains and iterations, especially useful in Quarto.\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effective sample size ratio and Rhat\nneff_ratio(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n     1.0474      1.0427      1.0071 \n```\n\n\n:::\n\n```{.r .cell-code}\nrhat(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n   0.999985    0.999927    1.000120 \n```\n\n\n:::\n:::\n\n\n\nThe effective sample size ratios are slightly above 1 and the R-hat values are very close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(bike_model, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-11-1.png){width=1152}\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_dens_overlay(bike_model)\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n\n## Simulation via `rstan`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nstan_bike_model <- \"\n  data {\n    int<lower = 0> n;\n    vector[n] Y;\n    vector[n] X;\n  }\n  parameters {\n    real beta0;\n    real beta1;\n    real<lower = 0> sigma;\n  }\n  model {\n    Y ~ normal(beta0 + beta1 * X, sigma);\n    beta0 ~ normal(-2000, 1000);\n    beta1 ~ normal(100, 40);\n    sigma ~ exponential(0.0008);\n  }\n\"\n```\n:::\n\n\n\n## Simulation via `rstan`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 2: SIMULATE the posterior\nstan_bike_sim <- \n  stan(model_code = stan_bike_model, \n       data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), \n       chains = 4, iter = 5000*2, seed = 84735)\n```\n:::\n\n\n\n\n\n\n## Posterior summary statistics\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  -2198.     354.    -2645.    -1742. \n2 temp_feel       82.2      5.03     75.7      88.6\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3488.      81.3    3383.     3592. \n```\n\n\n:::\n:::\n\n\n\nThe __posterior median relationship__\\index{posterior median relationship} is\n\n\\begin{equation}\n-2198.08 + 82.24 X.\n\\end{equation}\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store the 4 chains for each parameter in 1 data frame\nbike_model_df <- as.data.frame(bike_model)\n\n# Check it out\nnrow(bike_model_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20000\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(bike_model_df, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) temp_feel    sigma\n1   -2436.977  85.72491 1340.884\n2   -2102.929  80.88378 1227.981\n3   -1908.384  78.50958 1257.493\n```\n\n\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 50 simulated model lines\nbikes %>%\n  tidybayes::add_fitted_draws(bike_model, n = 50) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + \n    geom_point(data = bikes, size = 0.05)\n```\n\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n##\n\nDo we have ample posterior evidence that there's a positive association between ridership and temperature, i.e., that $\\beta_1 > 0$? \n\n__Visual evidence__ In our visual examination of 50 posterior plausible scenarios for the relationship between ridership and temperature, _all_ exhibited positive associations.\n\n__Numerical evidence from the posterior credible interval__ More rigorously, the 80% credible interval for $\\beta_1$ in  `tidy()` summary, (75.7, 88.6), lies entirely and well above 0.\n\n__Numerical evidence from a posterior probability__ A quick tabulation approximates that there's _almost certainly_ a positive association, $P(\\beta_1 > 0 \\; | \\; \\vec{y}) \\approx 1$. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tabulate the beta_1 values that exceed 0\nbike_model_df %>% \n  mutate(exceeds_0 = temp_feel > 0) %>% \n  janitor::tabyl(exceeds_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n exceeds_0     n percent\n      TRUE 20000       1\n```\n\n\n:::\n:::\n\n\n## Posterior prediction\n\nSuppose a weather report indicates that tomorrow will be a 75-degree day in D.C. What's your posterior guess of the number of riders that Capital Bikeshare should anticipate?\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nDo you think there will be 3970 riders tomorrow?\n\n$$-2198.08 + 82.24*75 = 3969.92 .$$\n\n## \n\nThere are two potential sources of variability:\n\n- __Sampling variability__ in the data    \n    The observed ridership outcomes, $Y$, typically _deviate_ from the model line. That is, we don't expect every 75-degree day to have the same exact number of rides.\n    \n- __Posterior variability__ in parameters $(\\beta_0, \\beta_1, \\sigma)$    \n    The posterior median model is merely the center in a _range_ of plausible model lines $\\beta_0 + \\beta_1 X$. We should consider this entire range as well as that in $\\sigma$, the degree to which observations might deviate from the model lines.\n\n## Posterior Predictive Model\n\nMathematically speaking:\n\n$$f\\left(y_{\\text{new}} | \\vec{y}\\right) = \\int\\int\\int f\\left(y_{new} | \\beta_0,\\beta_1,\\sigma\\right) f(\\beta_0,\\beta_1,\\sigma|\\vec{y}) d\\beta_0 d\\beta_1 d\\sigma .$$\n\nNow, we don't actually have a nice, tidy formula for the posterior pdf of our regression parameters, $f(\\beta_0,\\beta_1,\\sigma|\\vec{y})$, and thus can't get a nice tidy formula for the posterior predictive pdf $f\\left(y_{\\text{new}} | \\vec{y}\\right)$.\nWhat we _do_ have is 20,000 sets of parameters in the Markov chain $\\left(\\beta_0^{(i)},\\beta_1^{(i)},\\sigma^{(i)}\\right)$.\nWe can then _approximate_ the posterior predictive model for $Y_{\\text{new}}$ at $X = 75$ by simulating a ridership prediction from the Normal model evaluated each parameter set:\n\n$$Y_{\\text{new}}^{(i)} | \\beta_0, \\beta_1, \\sigma  \\; \\sim \\; N\\left(\\mu^{(i)}, \\left(\\sigma^{(i)}\\right)^2\\right) \\;\\; \\text{ with } \\;\\; \\mu^{(i)} = \\beta_0^{(i)} + \\beta_1^{(i)} \\cdot 75.$$\n\n##\n\nThus, each of the 20,000 parameter sets in our Markov chain (left) produces a unique prediction (right):\n\n$$\\left[\n\\begin{array}{lll} \n\\beta_0^{(1)} & \\beta_1^{(1)} & \\sigma^{(1)} \\\\\n\\beta_0^{(2)} & \\beta_1^{(2)} & \\sigma^{(2)} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\beta_0^{(20000)} & \\beta_1^{(20000)} & \\sigma^{(20000)} \\\\\n\\end{array}\n\\right]\n\\;\\; \\longrightarrow \\;\\;\n\\left[\n\\begin{array}{l} \nY_{\\text{new}}^{(1)} \\\\\nY_{\\text{new}}^{(2)} \\\\\n\\vdots \\\\\nY_{\\text{new}}^{(20000)} \\\\\n\\end{array}\n\\right]$$\n\nThe resulting collection of 20,000 predictions, $\\left\\lbrace Y_{\\text{new}}^{(1)}, Y_{\\text{new}}^{(2)}, \\ldots, Y_{\\text{new}}^{(20000)} \\right\\rbrace$, _approximates_ the posterior predictive model of ridership $Y$ on 75-degree days.\\index{posterior predictive model}\nWe will obtain this approximation both \"by hand,\" which helps us build some powerful intuition, and using shortcut R functions.\n\n\n## Building a posterior predictive model\n\nWe'll simulate 20,000 predictions of ridership on a 75-degree day, $\\left\\lbrace Y_{\\text{new}}^{(1)}, Y_{\\text{new}}^{(2)}, \\ldots, Y_{\\text{new}}^{(20000)} \\right\\rbrace$, one from each parameter set in `bike_model_df`.\nLet's start small with just the first posterior plausible parameter set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_set <- head(bike_model_df, 1)\nfirst_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) temp_feel    sigma\n1   -2436.977  85.72491 1340.884\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\nUnder this particular scenario, $\\left(\\beta_0^{(1)}, \\beta_1^{(1)}, \\sigma^{(1)}\\right) = (-2437, 85.72, 1341)$, the average ridership at a given temperature is defined by\n\n$$\\mu = \\beta_0^{(1)} + \\beta_1^{(1)} X = -2437 + 85.72X  .$$\n\n##\n\nAs such, we'd expect an __average__ of $\\mu = 3992$ riders on a 75-degree day:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- first_set$`(Intercept)` + first_set$temp_feel * 75\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3992.391\n```\n\n\n:::\n:::\n\n\n\n##\n\nTo capture the __sampling variability__ around this average, i.e., the fact that not all 75-degree days have the same ridership, we can simulate our first official prediction $Y_{\\text{new}}^{(1)}$ by taking a random draw from the Normal model specified by this first parameter set:\n\n$$Y_{\\text{new}}^{(1)} | \\beta_0, \\beta_1, \\sigma  \\; \\sim \\; N\\left(3992, 1341^2\\right)  .$$\n\n\n\n::: {.cell}\n\n:::\n\n\n\n. . .\n\nTaking a draw from this model using `rnorm()`, we happen to observe an above average 4887 rides on the 75-degree day:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\ny_new <- rnorm(1, mean = mu, sd = first_set$sigma)\ny_new\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4887.072\n```\n\n\n:::\n:::\n\n\n\n##\n\nNow let's do this 19,999 more times.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict rides for each parameter set in the chain\nset.seed(84735)\npredict_75 <- bike_model_df %>% \n  mutate(mu = `(Intercept)` + temp_feel*75,\n         y_new = rnorm(20000, mean = mu, sd = sigma))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict_75, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) temp_feel    sigma       mu    y_new\n1   -2436.977  85.72491 1340.884 3992.391 4887.072\n2   -2102.929  80.88378 1227.981 3963.355 3811.282\n3   -1908.384  78.50958 1257.493 3979.835 4961.771\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\nWhereas the collection of 20,000 `mu` values approximates the posterior model for the _typical_ ridership on 75-degree days, $\\mu = \\beta_0 + \\beta_1 * 75$, the 20,000 `y_new` values approximate the __posterior predictive model__ of ridership for tomorrow, an _individual_ 75-degree day, \\index{posterior predictive model}\n\n$$Y_{\\text{new}} | \\beta_0, \\beta_1, \\sigma  \\; \\sim \\; N\\left(\\mu, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu = \\beta_0 + \\beta_1 \\cdot 75 .$$\n\n##\n\nThe 95% credible interval for the __typical__ number of rides on a 75-degree day, $\\mu$, ranges from 3844 to 4094.\nIn contrast, the 95% __posterior prediction interval__ for the number of rides _tomorrow_ has a much _wider_ range from 1497 to 6512.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct 80% posterior credible intervals\npredict_75 %>% \n  summarize(lower_mu = quantile(mu, 0.025),\n            upper_mu = quantile(mu, 0.975),\n            lower_new = quantile(y_new, 0.025),\n            upper_new = quantile(y_new, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lower_mu upper_mu lower_new upper_new\n1 3844.319 4094.222  1497.287  6512.323\n```\n\n\n:::\n:::\n\n\n\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the posterior model of the typical ridership on 75 degree days\nggplot(predict_75, aes(x = mu)) + \n  geom_density()\n\n# Plot the posterior predictive model of tomorrow's ridership\nggplot(predict_75, aes(x = y_new)) + \n  geom_density()\n```\n:::\n\n\n\n##\n\nThe posterior model of $\\mu$, the typical ridership on a 75-degree day (left), and the posterior predictive model of the ridership tomorrow, a specific 75-degree day (right).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-fitting-regression_files/figure-revealjs/ch9-post-pred-1.png){fig-alt='There are two density plots of mu, both bell-shaped and centered at mu equals 3955. However, the left density plot is much narrower, ranging from roughly 3900 to 4100. The right density plot is wider, ranging from roughly 1500 to 6500.' width=960}\n:::\n:::\n\n\n\n\n\n## \n\nThere's more accuracy in anticipating the _average_ behavior across multiple data points than the _unique_ behavior of a single data point.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![95% posterior credible intervals (blue) for the __average__ ridership on 75-degree days (left) and the __predicted__ ridership for tomorrow, an individual 75-degree day (right).](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-31-1.png){fig-alt='There are two scatterplots of rides (y-axis) vs temperature (x-axis). Both display the original 500 data points. The left scatterplot is superimposed with a very short vertical line at a temp_feel of 75 -- it ranges from roughly 3800 to 4100 rides. The right scatterplot is superimposed with a much wider vertical line at a temp_feel of 75 -- it ranges from roughly 1500 to 6500 rides.' width=384}\n:::\n:::\n\n\n\n\n### Posterior prediction with rstanarm\n\nSimulating the posterior predictive model from scratch allowed you to really connect with the concept, but moving forward we can utilize the `posterior_predict()` function in the __rstanarm__ package: \\indexfun{posterior_predict()}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a set of predictions\nset.seed(84735)\nshortcut_prediction <- \n  posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))\n```\n:::\n\n\n\nThe `shortcut_prediction` object contains 20,000 predictions of ridership on 75-degree days.\nWe can both visualize and summarize the corresponding (approximate) posterior predictive model using our usual tricks.\nThe results are equivalent to those we constructed from scratch above:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct a 95% posterior credible interval\nposterior_interval(shortcut_prediction, prob = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      2.5%    97.5%\n1 1497.287 6512.323\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the approximate predictive model\nmcmc_dens(shortcut_prediction) + \n  xlab(\"predicted ridership on a 75 degree day\")\n```\n\n::: {.cell-output-display}\n![Posterior predictive model of ridership on a 75-degree day.](1-lec-fitting-regression_files/figure-revealjs/unnamed-chunk-33-1.png){fig-alt='A density plot of the predicted ridership on a 75-degree day. The density plot is bell-shaped, centered at roughly 4000 rides, and ranges from roughly 1000 to 7000.' width=10}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}