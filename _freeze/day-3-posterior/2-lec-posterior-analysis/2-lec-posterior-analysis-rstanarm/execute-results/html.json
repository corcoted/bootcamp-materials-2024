{
  "hash": "f983a0cb307c0ea9f1b3ffc45b32c7b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Posterior Analysis <br> with MCMC and rstanarm\"\nsubtitle: \"Day 3\"\nformat: \n  revealjs:\n    slide-number: true\n    incremental: true\n    theme: [\"../../templates/slides-style.scss\"]\n    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png\n    title-slide-attributes: \n      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png\n      data-background-size: 12%\n      data-background-position: 50% 95%\n---\n\n\n\n## Reaction Times\n\nWe consider data from a study (Belenky et al, 2003) of reaction time under a variety of sleep deprivation scenarios.  We will focus on the baseline values of reaction time, taken after study participants had a normal night's sleep. Suppose prior research has established a mean reaction time to a visual stimulus of 275ms, and we wish to determine whether the values in our study are consistent with this value. A first step is to read and plot the data.\n\n## Reaction Time Data\n\n::: panel-tabset\n\n## Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2-3|4-5\"}\nlibrary(tidyverse)\nlibrary(lme4) # lme4 package contains sleep study data\ndata(sleepstudy)\nbaseline <- sleepstudy %>%\n  filter(Days==2) # Day 2 is baseline, days 0-1 are training\nbaseline %>%\n  ggplot(aes(x=Reaction)) +\n  geom_histogram() + \n  labs(x=\"Reaction Time (ms)\") + \n  geom_vline(xintercept=275, col=\"red\")\n```\n:::\n\n\n\n## Plot\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-lec-posterior-analysis-rstanarm_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n\n\n## Analysis of Reaction Time\n\nWe can use the R brms() function to analyze the data. One advantage of this function is that it allows us easily to implement more complex sampling algorithms, beyond Gibbs sampling. \n\nSuppose our data model is $Y \\sim N(\\mu, \\sigma^2),$ and we wish to evaluate whether $\\mu$ is consistent with a reaction time of 275ms. We previously studied the conjugate normal-inverse gamma specification, given by $$\\mu \\mid \\sigma^2 \\sim N\\bigg(\\theta,\\frac{\\sigma^2}{n_0}\\bigg), ~~~\\frac{1}{\\sigma^2} \\sim \\text{gamma}\\bigg(\\frac{\\nu_0}{2},\\frac{\\nu_0\\sigma^2_0}{2}\\bigg).$$ While this prior has a nice interpretation in terms of prior samples, it's not the only option.  \n\n## Prior for the Mean\n\nInstead, we consider a relatively non-informative prior for our mean $\\mu \\sim N(300,50^2)$ -- based on a normal distribution, this puts 95% of the prior mass within 2 SD of the mean, which roughly corresponds to the range 200-400.  \n\n## Prior for the Standard Deviation\n\nWe will specify a prior directly on the standard deviation $\\sigma$ as $\\sigma \\sim \\text{HalfCauchy}(0,40)$, which has center 0 and scale parameter 40 and is called \"half\" because only the positive values are used. If you haven't seen the Cauchy distribution, it's a special case of a t distribution with 1 degree of freedom, and it's nice because it has fat tails and allows substantial mass away from its mean. We plot it here.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-lec-posterior-analysis-rstanarm_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Code to Fit Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- stan_glm(data = baseline, family = gaussian,\n                Reaction ~ 1, #only estimating a mean (Intercept) and variance\n                prior_intercept = normal(300, 50),\n                # prior for mu has mean 300 and sd 50\n                prior_aux = cauchy(0, 40),\n                # prior for sigma is half of a center 0, scale 40 Cauchy\n                chains = 4, iter = 5000*2, seed = 84735, refresh = FALSE\n)\n```\n:::\n\n\n\n\n\n## Checking the Output\n\nTrace plots can be used to check convergence. \n\n\n\n::: panel-tabset\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(fit) # show trace plots\n```\n:::\n\n\n\n\n## Plots\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-lec-posterior-analysis-rstanarm_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Checking the Output\n\n\n\n::: panel-tabset\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_dens_overlay(fit) # show posteriors\n```\n:::\n\n\n\n\n## Plots\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-lec-posterior-analysis-rstanarm_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Interpreting Trace Plots\n\nA trace plot depicts the MCMC samples by MCMC iteration number. If your MCMC algorithm has converged, we expect to see trace plots that reflect white noise around the target value. Indeed, that appears to be the case here.\n\n## Bad Trace Plots\n\n::: columns\n::: {.column width=\"60%\"}\n![Fig 6.12 in Bayes Rules](bad-trace-1.png)\n:::\n\n::: {.column width=\"40%\"}\nChain A has not yet converged to the right value (see black line in density plot), while Chain B is getting stuck and moving slowly around the sample space. In these cases, think carefully about your model and make sure it's appropriate, and if so, try sampling a larger number of iterations.\n:::\n:::\n\n\n\n\n## Comparing Sampled Values Across Chains\n\nWe also can check the $\\hat{R}$ values. These compare the sampled values across chains (we hope all the chains are giving similar results) by taking the square root of the ratio of the variability in the parameter of interest across all chains combined to the typical variability within a given chain. When $\\hat{R} \\approx 1$, we have stability across the chains. While there is no universal rule, often $\\hat{R}>1.05$ can be a cause for concern.\n\n## Checking $\\hat{R}$\n\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nrhat(fit)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       sigma \n   1.000134    1.000209 \n```\n\n\n:::\n:::\n\n\n\n## Effective Sample Size\n\nMarkov chain simulations in general produce samples that are correlated, rather than independent. The degree of autocorrelation in samples is related to the speed of convergence of the chain.  The *effective sample size (ESS)* is the number of *independent* samples you would need to get a similarly accurate posterior approximation. An effective sample size much smaller than the actual number of samples in a chain is an indication that our sampling is not very efficient. Depending on software courses, we may see one ESS for each parameter, or two values describing ESS in the center and tails of the distribution for each parameter.\n\n## Effective Sample Size\n\n-   The *bulk ESS* provides information about the sampling efficiency in the bulk of the distribution and is thus most relevant for providing information about efficiency of mean or median estimates. (brms)\n\n-   *Tail ESS* computes the minimum of ESS of the 5% and 95% quantiles and is useful for determining sampling efficiency in the tails and of the variance. (brms)\n\n-   *ESS* can also be calculated overall for each parameter (rstanarm)\n\n-   There are no hard and fast rules, but one recommendation is that both values should be at least 100 per Markov chain.\n\n-   Generally, we evalute quality of our samples based on all these criteria combined.\n\n## Effective Sample Size\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       sigma \n     0.5448      0.5251 \n```\n\n\n:::\n:::\n\n\n\n\n## Evaluating Our Hypothesis\n\nExamining the posterior distribution for $\\mu$, we see that most of its mass is below the value 275. \n\nWe can easily calculate the posterior probability that $\\mu<275$ by taking the fraction of posterior samples that are less than 275.\n\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nmean(as.data.frame(fit)$\"(Intercept)\"<275)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8917\n```\n\n\n:::\n:::\n\n\n\nSo the posterior probability that $\\mu<275$ is 0.8917.\n\n## Evaluating Our Hypothesis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit,\n     effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE,\n     conf.level = 0.95) # written summary of the output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    266.       6.97    252.      281. \n2 sigma           30.3      5.65     22.3      44.2\n3 mean_PPD       266.      10.4     246.      287. \n```\n\n\n:::\n:::\n\n\n\nWe can also use the 95% credible interval for $\\mu$ and note that 275 is inside this credible interval.\n\n## You Try It!\n\nThe priors specified are not very informative.  Try making the priors more informative and running the code on your own to see how the results are affected!\n\nFor example, you can\n- make the variance on the normal prior much smaller\n- make the scale on the half Cauchy prior much smaller\n\nTry changing just one aspect of the prior at a time to evaluate robustness.\n\n## Changes in Reaction Time with Sleep Deprivation\n\nNow that we've explored baseline reaction time, let's see what happens to the reaction times of study participants after one night of sleep deprivation (only 3 hours of time in bed).  First, we will do a little data wrangling and make a plot of the changse in reaction time, plotting the sleep deprived reaction time minus the baseline reaction time for each subject.\n\n## Changes in Reaction Time \n\n::: panel-tabset\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-2|3-4|5\"}\nsleepdep <- sleepstudy %>%\n  filter(Days %in% c(2,3)) %>%\n  pivot_wider(names_from = Days, names_prefix=\"Day\", \n              values_from = Reaction, id_cols=Subject) %>%\n  mutate(diff32=Day3-Day2)\n  # consider day 2 baseline and day 3, the first value from a sleep-deprived state, and calculate difference\n  # positive differences indicate longer reaction times when sleep-deprived\nsleepdep %>%\n  ggplot(aes(x=diff32)) +\n  geom_histogram() + \n  labs(x=\"Difference in Reaction Time (ms) when Sleep Deprived\") + \n  geom_vline(xintercept=0, col=\"red\")\n```\n:::\n\n\n\n## Plot\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-lec-posterior-analysis-rstanarm_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n\n## You Try It!\n\nUsing the previous slides as a guide, evaluate the hypothesis that sleep deprivation increases reaction time. \n\n::: callout-warning\nThink carefully about your priors!\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}