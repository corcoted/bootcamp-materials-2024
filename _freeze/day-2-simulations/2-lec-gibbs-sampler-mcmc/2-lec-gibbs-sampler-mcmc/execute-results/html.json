{
  "hash": "5e394d92abe542009a238c655f8ae23f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gibbs Sampler and MCMC\"\nsubtitle: \"Day 2\"\nformat: \n  revealjs:\n    slide-number: true\n    incremental: true\n    theme: [\"../../templates/slides-style.scss\"]\n    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png\n    title-slide-attributes: \n      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png\n      data-background-size: 12%\n      data-background-position: 50% 85%\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n# Recap: The Normal Model with One Unknown Parameter\n\n## The Normal-Normal Model for Unknown Mean\n\n-   The sampling model for $Y_1, \\cdots, Y_n$: \\begin{equation}\n      Y_1, \\cdots, Y_n \\mid \\mu, \\sigma \\overset{i.i.d.}{\\sim} \\textrm{Normal}(\\mu, \\sigma^2).\n      \\end{equation}\n\n-   The prior distribution for mean $\\mu$ ($\\sigma^2$ is known):\n    \\begin{equation}\n      \\mu \\mid {\\color{red}\\sigma} \\sim \\textrm{Normal}(\\theta, \\tau^2).\n      \\end{equation}\n\n-   The posterior distribution for mean $\\mu$ ($\\phi = 1/\\sigma^2$ and\n    $\\phi_0 = 1/\\tau^2$): \\begin{equation}\n      \\mu \\mid y_1, \\cdots, y_n, {\\color{red}\\phi} \\sim \\textrm{Normal}\\left(\\frac{\\phi_0 \\theta + n\\phi\\bar{y} }{\\phi_0 + n\\phi}, \\frac{1}{\\phi_0 + n \\phi}\\right)\n      \\end{equation}\n\n## The Normal-Normal Model for Unknown Mean\n\n-   The posterior distribution for mean $\\mu$: \\begin{equation}\n      \\mu \\mid y_1, \\cdots, y_n, {\\color{red}\\phi} \\sim \\textrm{Normal}\\left(\\frac{\\phi_0 \\theta + n\\phi\\bar{y} }{\\phi_0 + n\\phi}, \\frac{1}{\\phi_0 + n \\phi}\\right)\n      \\end{equation} where the precision $\\phi = 1/\\sigma^2$ and\n    $\\phi_0 = 1/\\tau^2$ which are known since $\\sigma^2$ and $\\tau^2$\n    are known\n\n-   We can then use the `rnorm()` R function to sample posterior draws\n    of $\\mu$ from its posterior distribution shown above (required\n    quantities: $\\theta, \\phi_0, \\bar{y}, \\phi$)\n\n## The Gamma-Normal Model for Unknown Variance\n\n-   The sampling model for $Y_1, \\cdots, Y_n$: \\begin{equation}\n      Y_1, \\cdots, Y_n \\mid \\mu, \\sigma \\overset{i.i.d.}{\\sim} \\textrm{Normal}(\\mu, \\sigma^2).\n      \\end{equation}\n\n-   The prior distribution for variance $\\sigma^2$ ($\\mu$ is known):\n    \\begin{equation}\n     1/\\sigma^2 \\mid {\\color{red}\\mu} \\sim \\textrm{Gamma}(\\alpha, \\beta).\n     \\end{equation}\n\n-   The posterior distribution for variance $\\sigma^2$: \\begin{equation}\n      1/\\sigma^2 \\mid y_1, \\cdots, y_n, {\\color{red}\\mu} \\sim \\textrm{Gamma} \\left(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\mu)^2 \\right)\n      \\end{equation}\n\n## The Gamma-Normal Model for Unknown Variance\n\n-   The posterior distribution for variance $\\sigma^2$: \\begin{equation}\n      1/\\sigma^2 \\mid y_1, \\cdots, y_n, {\\color{red}\\mu} \\sim \\textrm{Gamma} \\left(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\mu)^2 \\right)\n      \\end{equation}\n\n-   We can then use `rgamma()` R function to sample posterior draws of\n    $\\sigma^2$ from its posterior distribution shown above (required\n    quantities: $\\alpha, n, \\beta, \\{y_i\\}, \\mu$)\n\n# The Normal Model with Two Unknown Parameters\n\n## What If Both Parameters Are Unknown?\n\n::: nonincremental\n-   The sampling model for $Y_1, \\cdots, Y_n$: \\begin{equation}\n      Y_1, \\cdots, Y_n \\mid \\mu, \\sigma \\overset{i.i.d.}{\\sim} \\textrm{Normal}(\\mu, \\sigma^2).\n      \\end{equation}\n:::\n\n::: {.callout-warning icon=false}\n## Discussion question\n\nWhat if both $\\mu$ and $\\sigma^2$ are unknown? Given your Bayesian\ninference experience with either unknown $\\mu$ or unknown $\\sigma^2$,\nwhat would be a workflow for when both $\\mu$ and $\\sigma^2$ are unknown?\n:::\n\n## A Joint Prior Distribution for Mean and Variance\n\n-   Given what we have seen, how about a joint prior distribution as:\n    \\begin{equation}\n    f(\\mu, \\sigma^2) = f(\\mu) f(\\sigma^2)\n    \\end{equation}\n\n-   And let priors be \\begin{eqnarray}\n    \\mu &\\sim& \\textrm{Normal}(\\theta, \\tau^2)\\\\\n    1/\\sigma^2 &\\sim& \\textrm{Gamma}(\\alpha, \\beta)\n    \\end{eqnarray}\n\n## Full Conditional Posterior Distributions\n\n::: nonincremental\n-   Bayes' Theorem will produce two **full conditional posterior\n    distributions**:\n:::\n\n\n\n```{=tex}\n\\begin{eqnarray}\n\\mu \\mid y_1, \\cdots, y_n,{\\color{red}\\phi} &\\sim& \\textrm{Normal}\\left(\\frac{\\phi_0 \\theta + n\\phi\\bar{y}}{\\phi_0 + n \\phi}, \\frac{1}{\\phi_0 + n \\phi}\\right)  \\\\ \\nonumber \n1/\\sigma^2 = \\phi \\mid y_1, \\cdots, y_n,{\\color{red}\\mu} &\\sim& \\textrm{Gamma} \\left(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\mu)^2 \\right) \\nonumber \\\\\n\\end{eqnarray}\n```\n\n\n\n::: {.callout-warning icon=false}\n## Discussion question\nDo they look familiar? Without actual derivation, can you reason through\nthe derivation process to arrive at these two full conditional posterior\ndistributions?\n:::\n\n# Gibbs Samplers\n\n\n## Sampling Scheme: A Gibbs Sampler\n\n\n\n```{=tex}\n\\begin{eqnarray}\n\\mu \\mid y_1, \\cdots, y_n,{\\color{red}\\phi} &\\sim& \\textrm{Normal}\\left(\\frac{\\phi_0 \\theta + n\\phi\\bar{y}}{\\phi_0 + n \\phi}, \\frac{1}{\\phi_0 + n \\phi}\\right)  \\\\ \\nonumber \n1/\\sigma^2 = \\phi \\mid y_1, \\cdots, y_n,{\\color{red}\\mu} &\\sim& \\textrm{Gamma} \\left(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\mu)^2 \\right) \\nonumber \\\\\n\\end{eqnarray}\n```\n\n\nStart with initial values for parameters, $(\\mu^{(0)}, \\phi^{(0)})$. For\n$s = 1, \\ldots, S$, generate from the following sequence of **full\nconditional posterior distributions**:\n\n-   $\\mu^{(s)} \\sim f(\\mu \\mid \\phi^{(s-1)}, y_1, \\cdots, y_n)$\n\n-   $\\phi^{(s)} \\sim f(\\phi \\mid \\mu^{(s)}, y_1, \\cdots, y_n)$\n\n-   Set $\\theta^{(s)} = (\\mu^{(s)}, \\phi^{(s)})$\n\n## Sampling Scheme: A Gibbs Sampler\n\n::: nonincremental\n-   $\\mu^{(s)} \\sim f(\\mu \\mid \\phi^{(s-1)}, y_1, \\cdots, y_n)$\n\n-   $\\phi^{(s)} \\sim f(\\phi \\mid \\mu^{(s)}, y_1, \\cdots, y_n)$\n\n-   Set $\\theta^{(s)} = (\\mu^{(s)}, \\phi^{(s)})$\n\n-   The sequence $\\{\\theta^{(s)}$: $s = 1, \\ldots, S\\}$ may be viewed\n    (but is not necessarily... yet!) as a **dependent** sample from the\n    joint posterior distribution of $(\\mu, \\phi \\mid y_1, \\cdots, y_n)$\n:::\n\n## Writing A Gibbs Sampler as An R Function\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18\"}\ngibbs_normal <- function(input, S, seed){\n  set.seed(seed)\n  ybar <- mean(input$y)\n  n <- length(input$y)\n  para <- matrix(0, S, 2)\n  phi <- input$phi_init\n  for(s in 1:S){\n    theta1 <- (input$theta/input$tau^2 + n*phi*ybar)/\n    (1/input$tau^2 + n*phi)\n    tau1 <- sqrt(1/(1/input$tau^2 + n*phi))\n    mu <- rnorm(1, mean = theta1, sd = tau1)\n    alpha1 <- input$alpha + n/2\n    beta1 <- input$beta + sum((input$y - mu)^2)/2 \n    phi <- rgamma(1, shape = alpha1, rate = beta1)\n    para[s, ] <- c(mu, phi)\n  }\n  para \n}\n```\n:::\n\n\n\n::: {.callout-warning icon=false}\n## Discussion question\nWhat are the inputs and outputs of this `gibbs_normal()` function? Can\nyou see how the two full conditional posterior distributions get\ntranslated into lines of R code? Anything surprises you?\n:::\n\n## Features of Gibbs Samplers\n\n::: {.callout-warning icon=false}\n## Discussion question\nGiven our experience with the Gibbs sampler for the two-parameter normal model, what do you think are necessary to create a Gibbs sampler for sampling the posterior distribution? Do you expect to use a Gibbs sampler for any model and prior choices?\n:::\n\n## Extending to More Than Two Parameters\n\n::: {.callout-note icon=false}\n## Exercise\nSuppose the full conditional posterior distributions of\n$\\boldsymbol\\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_m)$ are all\ntractable (i.e., available through derivation). Describe how you can\ncreate a Gibbs sampler. Pay attention to the initial values and the\niterative process of a Gibbs sampler.\n:::\n\n# Overview of Markov Chain Monte Carlo\n\n## Markov Chain Monte Carlo (MCMC)\n\n- MCMC has been a standard practice of sampling the posterior\n\n- The term MCMC\n\n    - Markov chain: dependence, iterative\n    - Monte Carlo: sampling, independence\n\n- There are several popular MCMC algorithms\n\n    - Gibbs sampler is one example of MCMC algorithms\n    - Later we will introduce other MCMC algorithms, including Metropolis, Metropolis-Hastings, and Hamiltonian Monte Carlo\n\n## Markov Chain Monte Carlo (MCMC)\n\n- Creating and using MCMC\n    \n    - One can hand-code MCMC or use MCMC estimation software, including Stan (and various wrapper functions), JAGS, and BUGS\n    - The choice depends on teaching vs research, among other things\n\n- It is crucial to perform MCMC diagnostics **before** posterior analysis\n\n    - Posterior parameter draws are only useful if they converge to the true posterior\n    - Posterior analysis requires independent posterior draws\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}