{
  "hash": "fb6606223e1deed0bb141cae27066cb0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Conjugate Priors and their Posteriors\"\nsubtitle: \"Day 2\"\nformat: \n  revealjs:\n    slide-number: true\n    incremental: true\n    theme: [\"../../templates/slides-style.scss\"]\n    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png\n    title-slide-attributes: \n      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png\n      data-background-size: 12%\n      data-background-position: 50% 95%\n    include-after-body: ../../templates/clean_title_page.html\n---\n\n\n\n# {.center}\n\nNote that this lecture is based in part on [Chapter 5 of Bayes Rules! book](https://www.bayesrulesbook.com/chapter-5.html).\n\n# Review Application\n\n\n\n## California Drought\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/cadrought.jpeg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nDrought conditions have been a major concern for California residents and policy makers for years. We consider historical data on \"extremely dry years\" to formulate a prior distribution -- for example, from 1900-1999,  42 years were characterized as \"extremely dry.\" For data analysis, we will assume we go back in time to 2020 and let data from 2020, 2021, and 2022 contribute to our data likelihood.\n:::\n:::\n\n\n## Prior Distribution Options\n\nWe can use a beta prior based on the historical data from 1900-1999, during which we had 42 years that were extremely dry, and 58 years that were not. This corresponds to a beta(42,58) prior distribution (remember, the parameters of the beta can be interpreted as prior data: the approximate # of prior dry and non-dry years).  Let's plot and summarize this prior.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean      mode         var         sd\n1 0.42 0.4183673 0.002411881 0.04911091\n```\n\n\n:::\n:::\n\n\n\nDoes this seem reasonable?\n\n## Priors for Drought\n\nCalifornia weather has been more variable in recent years, and you may not all agree on an appropriate prior distribution. \n\n| Time                | Historical Data         | Prior Model       |\n|---------------------|--------------|-------------|\n| 1900-1999 | 42 extremely dry years           | Beta(42,58)  |\n| 2000-2009  | 9 extremely dry years | Beta(9,1)  |\n| 2010-2019  | 7 extremely dry years | Beta(7,3) |\n| NA | Vague prior not based on data | Beta(1,1) |\n\nPick one of these prior models, or design your own, and plot your prior. Share your plot with colleagues!\n\n## 2020\n\n::: {.panel-tabset}\n\n\n## Your Models\n\n| Prior Model | Data | Posterior Model |\n|---------|:----------:|-----------|\n| Beta(?,?) | Extremely dry year | Beta(?,?) |\n\n## Amy's Models\n\n|Prior Type | Prior Model | Data | Posterior Model |\n|:-------|---------|:----------:|-----------|\n| 1900-2019 | Beta(58, 62) | Extremely dry year | Beta(59,62) |\n| Last 10 years| Beta(7,3) | Extremely dry year | Beta(8,3) |\n| Agnostic | Beta(1,1) | Extremely dry year | Beta(2,1) |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(59,62)\nplot_beta(8,3)\nplot_beta(2,1)\n```\n:::\n\n\n\n## Results\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n## Aside: More Informative Prior\n\nFor the more informative prior, using 121 years of historical data, the prior and posterior are not exactly the same but are very close. We can focus in more closely to see the slight difference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(59, 62, y = 1, n = 1) +\n  labs(title = \"Prior Using 1900-2020\",y=NULL) + \n  xlim(0.3,0.675)\n```\n\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n## Aside: Posterior Proportional to Likelihood\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\nFor the uniform prior distribution, the prior takes the same value everywhere, so the posterior distribution (generally proportional to the prior times the likelihood) is simply proportional to the likelihood. \n\n\n\n## 2021\n\nNow, take your posterior and use it as the prior distribution for analysis of the data from 2021.\n\n\n## 2021\n\n::: {.panel-tabset}\n\n\n## Your Models\n\n| Prior Model | Data | Posterior Model |\n|---------|:----------:|-----------|\n| Beta(?,?) | Extremely dry year | Beta(?,?) |\n\n## Amy's Models\n\n|Prior Type | Prior Model | Data | Posterior Model |\n|:-------|---------|:----------:|-----------|\n| 1900-2020 | Beta(59, 62) | Extremely dry year | Beta(60,62) |\n| Last 11 years| Beta(8,3) | Extremely dry year | Beta(9,3) |\n| Agnostic | Beta(2,1) | Extremely dry year | Beta(3,1) |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(60,62)\nplot_beta(9,3)\nplot_beta(3,1)\n```\n:::\n\n\n\n## Results\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n## Aside: Posterior Proportional to Likelihood\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\nIn this case, the prior Beta(2,1) is proportional to $\\pi^{2-1}(1-\\pi)^{1-1}=\\pi$, and the likelihood is also proportional to $\\pi^1(1-\\pi)^0=\\pi$.  \n\n\n\n## 2022\n\nFinally, take your posterior and use it as the prior distribution for analysis of the data from 2022.\n\n## 2022\n\n::: {.panel-tabset}\n\n\n## Your Models\n\n| Prior Model | Data | Posterior Model |\n|---------|:----------:|-----------|\n| Beta(?,?) | Extremely dry year | Beta(?,?) |\n\n## Amy's Models\n\n|Prior Type | Prior Model | Data | Posterior Model |\n|:-------|---------|:----------:|-----------|\n| 1900-2021 | Beta(60, 62) | Extremely dry year | Beta(61,62) |\n| Last 12 years| Beta(9,3) | Extremely dry year | Beta(10,3) |\n| Agnostic | Beta(3,1) | Extremely dry year | Beta(4,1) |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(61,62)\nplot_beta(10,3)\nplot_beta(4,1)\n```\n:::\n\n\n\n## Results\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n# Continuous Response Data: The Normal-Normal Model\n\n## Data\n\nExample from [bayesrulesbook.com](https://bayesrulesbook.com)\n\nLet $(Y_1,Y_2,\\ldots,Y_{25})$ denote the hippocampal volumes for the $n = 25$ study subjects who played collegiate American football and who have been diagnosed with concussions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the data\nlibrary(bayesrules)\nlibrary(tidyverse)\ndata(football)\n\n# Filter the data\nfootball <- football %>%\n  filter(group == \"fb_concuss\") \n# Calcuate mean and sd of volumes\nfootball %>%\n  summarize(mean(volume), sd(volume))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(volume) sd(volume)\n1       5.7346  0.5933976\n```\n\n\n:::\n:::\n\n\n\n\n## Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(football, aes(x = volume)) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## The Normal Model\n\nLet $Y$ be a random variable which can take any value between $-\\infty$ and $\\infty$, ie. $Y \\in (-\\infty,\\infty)$.  Then the variability in $Y$ might be well represented by a Normal model with **mean parameter** $\\mu \\in (-\\infty, \\infty)$ and **standard deviation parameter** $\\sigma > 0$: \n\n$$Y \\sim N(\\mu, \\sigma^2)$$\n\nThe Normal model is specified by continuous pdf\n\n\\begin{equation}\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg[{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\bigg] \\;\\; \\text{ for } y \\in (-\\infty,\\infty)\n\\end{equation}\n\n\n\n\n## Trends and Variability of the Normal Model\n\n$$\\begin{split}\nE(Y) & = \\text{ Mode}(Y) = \\mu \\\\\n\\text{Var}(Y) & = \\sigma^2 \\\\\n\\end{split}$$\n\nFurther, $\\sigma$ provides a sense of scale for $Y$. Roughly 95% of $Y$ values will be within 2 standard deviations of $\\mu$:\n\n\\begin{equation}\n\\mu \\pm 2\\sigma \\; .\n\\end{equation}\n\n\n\n\n\n## Normal models\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/normal-tuning-1.png){width=1152}\n:::\n:::\n\n\n\n\n\n## Normal Likelihood\n\n$$L(\\mu |\\vec{y})= \\prod_{i=1}^{n}L(\\mu | y_i) = \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg[{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\\bigg].$$\n\nSimplifying this up to a _proportionality_ constant\n\n$$L(\\mu |\\vec{y}) \\propto \\prod_{i=1}^{n} \\exp\\bigg[{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\\bigg] =  \\exp\\bigg[{-\\frac{\\sum_{i=1}^n (y_i-\\mu)^2}{2\\sigma^2}}\\bigg] \\; .$$\n\n. . . \n\n\\begin{equation}\nL(\\mu | \\vec{y}) \\propto \\exp\\bigg[{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\\bigg] \\;\\;\\;\\; \\text{ for } \\; \\mu \\in (-\\infty, \\infty).\n\\end{equation}\n\n## \n\n$$L(\\mu | \\vec{y}) \\propto \\exp\\bigg[{-\\frac{(5.735-\\mu)^2}{2(0.593^2/25)}}\\bigg] \\;\\;\\;\\; \\text{ for } \\; \\mu \\in (-\\infty, \\infty),$$ \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The joint Normal likelihood function for the mean hippocampal volume.](1-lec-posterior-conjugate_files/figure-revealjs/likelihood-hippocampus-ch5-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Normal Prior for the Mean\n\n$$\\mu \\sim N(\\theta, \\tau^2) \\; , $$\n\nwith prior pdf \n\n\\begin{equation}\nf(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\bigg[{-\\frac{(\\mu - \\theta)^2}{2\\tau^2}}\\bigg] \\;\\; \\text{ for } \\mu \\in (-\\infty,\\infty) \\; .\n\\end{equation}\n\n\n. . . \n\n[Wikipedia](https://en.wikipedia.org/wiki/Hippocampus) tells us that among the general population of human adults, both halves of the hippocampus have a volume between 3.0 and 3.5 cubic centimeters.\nThus the _total_ hippocampal volume of _both_ sides of the brain is between 6 and 7 cubic centimeters. A standard deviation $\\sigma=0.5$ for a normal prior centered at 6.5 puts about 2/3 of the mass of the prior in this range:\n$\\mu \\sim N(6.5, 0.5^2) \\;.$\n\n## $N(6.5, 0.5^2)$ Distribution\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## The Normal-Normal Bayesian Model\n\nLet $\\mu \\in (-\\infty,\\infty)$ be an unknown _mean_ parameter and $(Y_1,Y_2,\\ldots,Y_n)$ be an independent $N(\\mu,\\sigma^2)$ sample where $\\sigma$ is assumed to be *known* (we'll relax this strong assumption shortly).\nThe Normal-Normal Bayesian model complements the Normal structure of the data with a Normal prior on $\\mu$:\n\n$$\\begin{split}\nY_i | \\mu & \\stackrel{ind}{\\sim} N(\\mu, \\sigma^2) \\\\\n\\mu & \\sim N(\\theta, \\tau^2) \\\\\n\\end{split}$$\n\nUpon observing data $\\vec{y} = (y_1,y_2,\\ldots,y_n)$, the posterior model of $\\mu$ is $N(\\mu_n,\\tau^2_n)$.\n\n## Normal Posterior\n\nIn the posterior $N(\\mu_n,\\tau^2_n)$, we have\n$$\\mu_n=\\frac{\\bigg(\\frac{1}{\\tau^2}\\bigg)\\theta+\\bigg(\\frac{n}{\\sigma^2}\\bigg)\\bar{y}}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}$$ and $$\\tau^2_n=\\frac{1}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}.$$\n\n\n. . . \n\nThe **inverse variance** $\\frac{1}{\\tau^2_n}$ is also called the **precision**. Note $\\frac{1}{\\tau^2_n}=\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}$ and thus the posterior precision is the prior precision (inverse variance) combined with the inverse data variance (information).\n\n## Posterior Mean as Weighted Average of Prior and Sample Means\n\nNow  $$\\mu_n=\\frac{\\bigg(\\frac{1}{\\tau^2}\\bigg)\\theta+\\bigg(\\frac{n}{\\sigma^2}\\bigg)\\bar{y}}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}.$$\n\n\nNotice that the posterior mean is a weighted average of the prior mean and the sample mean, with the prior mean weighted by the prior precision, and the sample mean weighted by its sampling precision.\n\n\n## Back to Brains\n\nFor the hippocampus data, we selected the prior $\\mu \\sim N(6.5, 0.5^2)$, and recall in our dataset the mean was 5.735 and sd was 0.593.\n\nThe posterior model of $\\mu$ is:\n\n$$\\mu | \\vec{y} \\; \\sim \\; N\\bigg(\\frac{6.5*\\frac{1}{0.5^2} + 5.735*\\frac{25}{0.593^2}}{\\frac{1}{0.5^2}+\\frac{25}{0.593^2}}, \\; \\frac{1}{\\frac{1}{0.5^2}+\\frac{25}{0.593^2}}\\bigg)\\;,$$\n\nor, further simplified,\n\n$$\\mu | \\vec{y} \\; \\sim \\; N\\bigg(5.776, 0.115^2 \\bigg) \\; .$$\n\n## \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/unnamed-chunk-16-1.png){width=864}\n:::\n:::\n\n\n\n## {.center}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      model     mean     mode        var        sd\n1     prior 6.500000 6.500000 0.25000000 0.5000000\n2 posterior 5.775749 5.775749 0.01331671 0.1153981\n```\n\n\n:::\n:::\n\n\n\n\n## Interpretation of Conjugate Prior in Terms of Historical Data {.scrollable}\n\nIf we base our prior mean on $n_0$ observations from a similar population as $(Y_1,Y_2,\\ldots,Y_n)$, we may wish to set $\\tau^2=\\frac{\\sigma^2}{n_0}$, the variance of the mean of the prior observations. In this case, we can simplify our equation as \n\n$$\\mu_n=\\frac{\\bigg(\\frac{1}{\\tau^2}\\bigg)\\theta+\\bigg(\\frac{n}{\\sigma^2}\\bigg)\\bar{y}}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}} = \\frac{\\bigg(\\frac{n_0}{\\sigma^2}\\bigg)\\theta+\\bigg(\\frac{n}{\\sigma^2}\\bigg)\\bar{y}}{\\frac{n_0}{\\sigma^2}+\\frac{n}{\\sigma^2}}$$\n$$\\mu_n= \\bigg(\\frac{n_0}{n_0+n}\\bigg)\\theta + \\bigg(\\frac{n}{n_0+n}\\bigg)\\bar{y},$$\nso we see our conjugate prior has an interpretation in terms of historical data when $\\theta$ is the prior mean in a sample of size $n_0$.\n\n\n## Known $\\sigma^2$?\n\nWhen is $\\sigma^2$ actually known?  Well, pretty much never.  This case is primarily used for didactic purposes.\n\n## Conjugate Inference in Normal-Normal Model with $\\sigma^2$ Unknown\n\nTypically, we make joint inference for the mean and variance. Using probability axioms, we express a joint distribution as a product of a conditional probability and a marginal probability, e.g. $$f(\\mu,\\sigma^2)=f(\\mu \\mid \\sigma^2)f(\\sigma^2).$$ \n\n## Prior for $\\sigma^2$\n\nFor $\\sigma^2$, we need a prior with support on $(0,\\infty)$. One nice option is the gamma distribution. While this distribution is not conjugate for $\\sigma^2$, it is conjugate for the precision, $\\frac{1}{\\sigma^2}$\n\n. . .\n\nThe gamma distribution is parameterized in terms of a shape parameter $\\alpha$ and rate parameter $\\beta$ and has mean $\\frac{\\alpha}{\\beta}$ and variance $\\frac{\\alpha}{\\beta^2}$.\n\nWe write these prior models as $\\frac{1}{\\sigma^2} \\sim \\text{gamma}(\\alpha,\\beta)$ or equivalently $\\sigma^2 \\sim \\text{inverse-gamma}(\\alpha,\\beta)$.\n\n\n## Gamma Models\n\nSuppose we think the variance should be centered around $\\frac{1}{2}$. Here are some gamma priors (for the precision) all with mean $2=\\frac{1}{0.5}$. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1-lec-posterior-conjugate_files/figure-revealjs/gamma-tuning-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Interpretable Gamma Prior Model for Precision\n\nRecall the $\\text{gamma}(\\alpha,\\beta)$ prior has mean $\\frac{\\alpha}{\\beta}$.  One way to pick $\\alpha$ and $\\beta$ would be to center the prior on an existing estimate of the precision. So for interpretability, we can set $$\\frac{1}{\\sigma^2} \\sim \\text{gamma}\\bigg(\\frac{\\nu_0}{2},\\frac{\\nu_0\\sigma^2_0}{2}\\bigg),$$ where we interpret $\\sigma^2_0$ as the sample variance from a sample of prior observations of size $\\nu_0$.  \n\n. . . \n\nThis prior for the precision has mean $\\frac{1}{\\sigma^2_0}$.\n\n## Normal-Inverse Gamma Conjugate Prior Model\n\nWhen our priors are $$\\frac{1}{\\sigma^2} \\sim \\text{gamma}\\bigg(\\frac{\\nu_0}{2},\\frac{\\nu_0\\sigma^2_0}{2}\\bigg)$$  and $$\\mu \\mid \\sigma^2 \\sim N\\bigg(\\theta,\\frac{\\sigma^2}{n_0}\\bigg), $$ and our data model is $$Y_1, \\ldots, Y_n \\mid \\mu, \\sigma^2 \\sim N(\\mu,\\sigma^2)... $$ \n\n## Normal-Inverse Gamma Conjugate Prior Model\n... we have a conjugate prior for the variance and a *conditionally-conjugate* prior for the mean. The **full conditional posterior** for $\\mu$ is  $$\\mu \\mid Y_1, \\ldots, Y_n, \\sigma^2 \\sim N\\bigg(\\mu_n, \\frac{\\sigma^2}{n*}\\bigg),$$ where $n^*=n_0+n$ and $\\mu_n=\\frac{\\frac{n_0}{\\sigma^2}\\theta+\\frac{n}{\\sigma^2}\\bar{y}}{\\frac{n_0}{\\sigma^2}+\\frac{n}{\\sigma^2}}=\\frac{n_0\\theta+n\\bar{y}}{n^*}.$\n\nWith $\\theta$ viewed as the mean of $n_0$ prior observations, then $E(\\mu \\mid Y_1, \\ldots, Y_n,\\sigma^2)$ is the sample mean of the current and prior observations, and $\\text{Var}(\\mu \\mid Y_1, \\ldots, Y_n, \\sigma^2)$ is $\\sigma^2$ divided by the total number of observations (prior+current).\n\n## Normal-Inverse Gamma Conjugate Prior Model\n\nTo get the **marginal posterior** of $\\sigma^2$, we  integrate over the unknown value of $\\mu$ and obtain $$\\frac{1}{\\sigma^2} \\mid Y_1, \\ldots, Y_n \\sim \\text{gamma}\\bigg(\\frac{\\nu_n}{2},\\frac{\\nu_n \\sigma^2_n}{2}\\bigg),$$ where $\\nu_n=\\nu_0+n$, $$\\sigma^2_n=\\frac{1}{\\nu_n}\\bigg[\\nu_0\\sigma^2_0 + (n-1)s^2 + \\frac{n_0n}{n^*}(\\bar{y}-\\theta)^2\\bigg],$$ where $s^2=\\frac{\\sum_{i=1}^n (y_i-\\bar{y})^2}{n-1}$ is the sample variance. \n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}